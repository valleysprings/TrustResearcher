# LLM Configuration
llm:
  # model_name: "gpt-5.2-chat-latest"
  # model_name: "deepseek-v3.2"
  model_name: "gemini-3-flash-preview"
  api_key: "YOUR_API_KEY_HERE"
  base_url: "YOUR_BASE_URL_HERE"
  max_tokens: 32768  # max tokens for input + output
  temperature: 0.8
  # Concurrency and rate limiting
  max_concurrent_calls: 8  # Maximum concurrent API calls
  rate_limit_calls: 30  # Maximum calls per time window
  rate_limit_window: 60  # Time window in seconds
  # Timeout settings for different operations
  timeouts:
    default: 300  # 5 minutes
    session_timeout: 120  # 2 minutes for HTTP session
    idea_generation: 1800  # 30 minutes for complex generation
    variant_generation: 900  # 15 minutes for variants
    cross_pollination: 600  # 10 minutes for cross-pollination
    merging: 120  # 2 minutes for idea merging
  # Token cost tracking
  token_cost:
    enabled: true
    words_per_token: 0.75  # Estimation: 1 token â‰ˆ 0.75 words (English only)
    # Custom model pricing (USD per 1M tokens)
    custom_pricing:
      prompt: 0.75  # $0.75 per 1M input tokens
      completion: 3.00  # $3.00 per 1M output tokens
  # Retry settings
  retry:
    max_retries: 1  # Maximum number of retries for failed API calls
    delay: 2  # seconds between retries
    timeout: 120  # Timeout for each API call (in seconds)
